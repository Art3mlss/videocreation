{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2397e835-739e-4122-8650-801f7baf7215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "     ---------------------------------------- 0.0/800.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 800.5/800.5 kB 16.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numba (from openai-whisper)\n",
      "  Using cached numba-0.60.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting numpy (from openai-whisper)\n",
      "  Using cached numpy-2.1.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting torch (from openai-whisper)\n",
      "  Downloading torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting tqdm (from openai-whisper)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper)\n",
      "  Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting numpy (from openai-whisper)\n",
      "  Using cached numpy-2.0.2-cp310-cp310-win_amd64.whl.metadata (59 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Collecting filelock (from torch->openai-whisper)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from torch->openai-whisper) (4.11.0)\n",
      "Collecting networkx (from torch->openai-whisper)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from torch->openai-whisper) (3.1.4)\n",
      "Collecting fsspec (from torch->openai-whisper)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch->openai-whisper)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->openai-whisper)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Using cached numpy-2.0.2-cp310-cp310-win_amd64.whl (15.9 MB)\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 884.2/884.2 kB 38.9 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 8.7/203.1 MB 41.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 17.8/203.1 MB 44.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 26.7/203.1 MB 43.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 35.9/203.1 MB 44.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 44.8/203.1 MB 44.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 54.0/203.1 MB 44.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 62.7/203.1 MB 44.3 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 71.8/203.1 MB 44.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 81.3/203.1 MB 44.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 89.4/203.1 MB 44.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 98.3/203.1 MB 44.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 106.4/203.1 MB 43.8 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 114.8/203.1 MB 43.9 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 124.5/203.1 MB 43.9 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 133.7/203.1 MB 44.0 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 142.6/203.1 MB 44.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 151.8/203.1 MB 44.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 161.5/203.1 MB 44.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 170.4/203.1 MB 44.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.8/203.1 MB 44.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 187.4/203.1 MB 44.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.6/203.1 MB 44.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 44.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 42.0 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803358 sha256=135c007a8ccba274f4bfd72f97cbbc080cf01e92f84bccc56dcf55ea653acf62\n",
      "  Stored in directory: c:\\users\\clemb\\appdata\\local\\pip\\cache\\wheels\\dd\\4a\\1f\\d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: mpmath, tqdm, sympy, regex, numpy, networkx, more-itertools, llvmlite, fsspec, filelock, torch, tiktoken, numba, openai-whisper\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 llvmlite-0.43.0 more-itertools-10.5.0 mpmath-1.3.0 networkx-3.4.2 numba-0.60.0 numpy-2.0.2 openai-whisper-20240930 regex-2024.11.6 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1 tqdm-4.67.0\n",
      "Requirement already satisfied: syllapy in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: pysrt in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from pysrt) (5.2.0)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (from opencv-python) (2.0.2)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Requirement already satisfied: numpy in c:\\users\\clemb\\anaconda3\\envs\\tik_tok\\lib\\site-packages (2.0.2)\n",
      "Collecting pillow\n",
      "  Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Using cached pillow-11.0.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-11.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install openai-whisper\n",
    "#!pip install syllapy\n",
    "#!pip install pysrt\n",
    "#!pip install opencv-python\n",
    "#!pip install numpy\n",
    "#!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623c0a23-2795-4c07-8953-f51c3c1bf408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import syllapy\n",
    "import pysrt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from PIL import ImageFont, ImageDraw, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b790fcc-948d-4349-b185-5d4d69f4dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables_syllapy(word):\n",
    "    return syllapy.count(word)\n",
    "\n",
    "# Séparer une phrase en mots et calculer les syllabes pour chaque mot\n",
    "def get_word_syllables(sentence):\n",
    "    words = sentence.split()\n",
    "    syllables_per_word = [count_syllables_syllapy(word) for word in words]\n",
    "    return words, syllables_per_word\n",
    "\n",
    "# Distribuer le temps d'une phrase entre les mots et les syllabes\n",
    "def distribute_time_over_words_and_syllables(start_time, end_time, syllables_per_word):\n",
    "    total_syllables = sum(syllables_per_word)\n",
    "    total_time = end_time - start_time\n",
    "    time_per_syllable = total_time / total_syllables\n",
    "\n",
    "    syllable_times = []\n",
    "    current_time = start_time\n",
    "\n",
    "    for syllables in syllables_per_word:\n",
    "        word_duration = syllables * time_per_syllable\n",
    "        syllable_times.append((syllables, current_time, current_time + word_duration))\n",
    "        current_time += word_duration\n",
    "\n",
    "    return syllable_times\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871af74d-820e-4f1f-a68b-8c5631362557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_srt_file(karaoke_times, output_srt_file):\n",
    "    with open(output_srt_file, 'w', encoding='utf-8') as f:\n",
    "        for i, (sentence, start_time, end_time, syllable_times) in enumerate(karaoke_times):\n",
    "            for word_index, (word, syllable_count, word_end_time) in enumerate(syllable_times):\n",
    "                start_time_formatted = format_srt_time(start_time)\n",
    "                end_time_formatted = format_srt_time(word_end_time)\n",
    "                \n",
    "                # SRT entry format\n",
    "                f.write(f\"{i+1}\\n\")\n",
    "                f.write(f\"{start_time_formatted} --> {end_time_formatted}\\n\")\n",
    "                f.write(f\"{word}\\n\\n\")\n",
    "                \n",
    "                # Update start time for next word\n",
    "                start_time = word_end_time\n",
    "\n",
    "def format_srt_time(seconds):\n",
    "    millis = int((seconds % 1) * 1000)\n",
    "    seconds = int(seconds)\n",
    "    minutes = seconds // 60\n",
    "    seconds = seconds % 60\n",
    "    hours = minutes // 60\n",
    "    minutes = minutes % 60\n",
    "    return f\"{hours:02}:{minutes:02}:{seconds:02},{millis:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d7363d-3961-4b0f-9eea-1b99ac6a4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcrire l'audio et générer les timings\n",
    "def transcribe_audio_to_karaoke_times(audio_file):\n",
    "    model = whisper.load_model(\"tiny\", device='cuda')\n",
    "    result = model.transcribe(audio_file)\n",
    "    \n",
    "    sentence_times_global = []\n",
    "    previous_end_time = 0\n",
    "\n",
    "    for segment in result['segments']:\n",
    "        sentence = segment['text']\n",
    "        start_time = segment['start']\n",
    "        end_time = segment['end']\n",
    "        \n",
    "        if start_time > previous_end_time:\n",
    "            start_time = previous_end_time\n",
    "        \n",
    "        words, syllables_per_word = get_word_syllables(sentence)\n",
    "        syllable_times = distribute_time_over_words_and_syllables(start_time, end_time, syllables_per_word)\n",
    "        syllables_durations = list(zip(words, syllables_per_word, [word_end for _, _, word_end in syllable_times]))\n",
    "        sentence_times_global.append((sentence, start_time, end_time, syllables_durations))\n",
    "        \n",
    "        previous_end_time = end_time\n",
    "    \n",
    "    return sentence_times_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769a40b3-6ac0-4e42-9c2e-36c8639b11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier SRT\n",
    "def load_srt_file(srt_file):\n",
    "    subs = pysrt.open(srt_file)\n",
    "    word_times = []\n",
    "    for sub in subs:\n",
    "        word_times.append((sub.text.strip(), sub.start.ordinal / 1000, sub.end.ordinal / 1000))\n",
    "    return word_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2c0f8b8-3070-427a-bab2-44074aa6e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter du texte sur les frames\n",
    "def render_text_on_frame(frame, text, highlighted_words, font_path, font_size, pos=(300, 270), font_color=(255, 255, 255), highlight_color=(255, 0, 0), bg_color=(0, 0, 0), padding=7, y_offset_adjust=25, right_adjust=10, bg_opacity=255):\n",
    "    # Convertir la frame en image Pillow avec un canal alpha (transparence)\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).convert(\"RGBA\")\n",
    "    \n",
    "    # Créer une image temporaire pour dessiner le rectangle avec transparence\n",
    "    overlay = Image.new(\"RGBA\", pil_image.size, (0, 0, 0, 0))  # Image transparente\n",
    "    overlay_draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    # Charger la police spécifiée\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    \n",
    "    # Séparer les mots\n",
    "    words = text.split()\n",
    "\n",
    "    # Calculer la largeur et hauteur totale du texte\n",
    "    text_width = 0\n",
    "    text_height = 0\n",
    "    word_widths = []  # Pour stocker la largeur de chaque mot\n",
    "\n",
    "    # Calculer les dimensions de tous les mots\n",
    "    for word in words:\n",
    "        word_bbox = overlay_draw.textbbox((0, 0), word, font=font)\n",
    "        word_width = word_bbox[2] - word_bbox[0]\n",
    "        word_height = word_bbox[3] - word_bbox[1]\n",
    "        word_widths.append(word_width)  # Stocke la largeur du mot\n",
    "        text_width += word_width + 10  # Ajoute un espace entre les mots\n",
    "        text_height = max(text_height, word_height)  # Prendre la plus grande hauteur de mot\n",
    "\n",
    "    # Enlève le dernier espace supplémentaire\n",
    "    text_width -= 10\n",
    "\n",
    "    # Calculer la position (x_offset, y_offset) pour centrer le texte en bas\n",
    "    frame_width, frame_height = pil_image.size\n",
    "    x_offset = (frame_width - text_width) // 2  # Centrer horizontalement\n",
    "    y_offset = frame_height - text_height - 50 + y_offset_adjust  # Placer plus bas avec ajustement\n",
    "\n",
    "    # Dessiner le fond noir semi-transparent derrière le texte\n",
    "   \n",
    "   #overlay_draw.rectangle(\n",
    "  #      [x_offset + padding, y_offset - padding // 2, x_offset + text_width - padding, y_offset + text_height + padding // 2],\n",
    "   #     fill=(*bg_color, bg_opacity)  # Ajouter l'opacité ici\n",
    "    #)\n",
    "\n",
    "    # Ajuster l'alignement vertical pour corriger le positionnement\n",
    "    #text_bbox = overlay_draw.textbbox((0, 0), text, font=font)\n",
    "    #y_text_offset = y_offset + (text_height - (text_bbox[3] - text_bbox[1])) // 2\n",
    "\n",
    "\n",
    "    # Dessiner les mots avec les ajustements\n",
    "    word_x = x_offset\n",
    "    for i, word in enumerate(words):\n",
    "        word_color = highlight_color if word in highlighted_words else font_color\n",
    "       # overlay_draw.text((word_x, y_text_offset), word, font=font, fill=word_color)\n",
    "        word_x += word_widths[i] + 10  # Ajuste l'espace pour chaque mot\n",
    "\n",
    "    # Combiner l'image avec l'overlay semi-transparent\n",
    "    combined = Image.alpha_composite(pil_image, overlay)\n",
    "\n",
    "    # Convertir en frame OpenCV\n",
    "    frame = cv2.cvtColor(np.array(combined.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
    "    return frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f886802-97d4-482f-a660-dd72b10e70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer une vidéo avec les sous-titres karaoke en utilisant le fichier SRT\n",
    "def create_video_with_karaoke_using_srt(input_video, srt_file, output_video, font_path, font_size):\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    word_times = load_srt_file(srt_file)\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    temp_output_video = 'temp_output.mp4'  # Temp video without audio\n",
    "    out = cv2.VideoWriter(temp_output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    current_word_index = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or current_word_index >= len(word_times):\n",
    "            break\n",
    "        \n",
    "        current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "\n",
    "        word, start_time, end_time = word_times[current_word_index]\n",
    "        \n",
    "        if start_time <= current_time <= end_time:\n",
    "            frame = render_text_on_frame(frame, word, [word], font_path, font_size)\n",
    "        \n",
    "        if current_time > end_time:\n",
    "            current_word_index += 1\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    return temp_output_video  # Return temp video without audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "697823d9-8bfa-47ae-998b-0418ec1e636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction pour ajouter l'audio à la vidéo en utilisant FFmpeg\n",
    "def combine_video_and_audio(input_video, input_audio, output_video):\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_video,                  # Input video (avec les sous-titres déjà intégrés)\n",
    "        '-i', input_audio,                  # Input audio\n",
    "        '-c:v', 'copy',                     # Ne pas réencoder la vidéo\n",
    "        '-c:a', 'aac',                      # Codec audio AAC\n",
    "        '-b:a', '192k',                     # Débit audio\n",
    "        '-shortest',                        # Faire en sorte que la vidéo s'arrête quand l'audio ou la vidéo le plus court se termine\n",
    "        output_video                        # Fichier de sortie\n",
    "    ]\n",
    "    \n",
    "    # Exécuter la commande FFmpeg\n",
    "    subprocess.run(ffmpeg_command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a8bab-84ec-42d4-bd2b-47aba1771cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
